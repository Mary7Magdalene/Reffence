{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mary7Magdalene/Reffence/blob/main/implementation_of_forward_propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biRXdvZH-vEQ"
      },
      "outputs": [],
      "source": [
        "#importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#creating a dataset\n",
        "# Creating a dataset\n",
        "df = pd.DataFrame([[8, 8, 4], [7, 9, 5], [6, 10, 6], [5, 12, 7]], columns=['cgpa', 'profile_score', 'lpa'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFLFRzdB_y4n"
      },
      "outputs": [],
      "source": [
        "#initializing the parameters\n",
        "def initialize_parameters(layer_dims):\n",
        "    np.random.seed(3)  # Setting seed for reproducibility\n",
        "    parameters = {}  # Dictionary to store weights and biases\n",
        "    L = len(layer_dims)  # Number of layers in the network\n",
        "\n",
        "    for i in range(1, L):\n",
        "        # Initializing weights with small constant values (0.1) instead of random values\n",
        "        parameters['W' + str(i)] = np.ones((layer_dims[i-1], layer_dims[i])) * 0.1\n",
        "        # Initializing biases with zeros\n",
        "        parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
        "\n",
        "    return parameters  # Returns the initialized parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements the forward pass of a single linear layer in the neural network.\n",
        "A_prev: Activations from the previous layer.\n",
        "W: Weights of the current layer.\n",
        "b: Biases of the current layer.\n",
        "Returns Z, which is the weighted sum of inputs plus bias."
      ],
      "metadata": {
        "id": "swwXPNvoOyiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A_prev, W, b):\n",
        "    Z = np.dot(W.T, A_prev) + b  # Computing the linear transformation Z = W^T * A_prev + b\n",
        "    return Z\n"
      ],
      "metadata": {
        "id": "n-LbKzizOl_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU Activation Function"
      ],
      "metadata": {
        "id": "RCeOMB0KPpQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z):\n",
        "    return np.maximum(0, Z)  # Applying ReLU activation function\n"
      ],
      "metadata": {
        "id": "XJwESntJO3fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation through L Layers"
      ],
      "metadata": {
        "id": "_n4OM7fZPxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_layer_forward(X, parameters):\n",
        "    A = X  # Input data\n",
        "    caches = []  # List to store intermediate computations\n",
        "    L = len(parameters) // 2  # Number of layers in the network (excluding input layer)\n",
        "\n",
        "    # Forward propagation through hidden layers\n",
        "    for i in range(1, L):\n",
        "        A_prev = A  # Storing the previous activation\n",
        "        W = parameters['W' + str(i)]  # Fetching weights of current layer\n",
        "        b = parameters['b' + str(i)]  # Fetching biases of current layer\n",
        "        Z = linear_forward(A_prev, W, b)  # Computing linear transformation\n",
        "        A = relu(Z)  # Applying ReLU activation\n",
        "        cache = (A_prev, W, b, Z)  # Storing values for backpropagation (not used here)\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Output layer (no activation function applied)\n",
        "    W_out = parameters['W' + str(L)]\n",
        "    b_out = parameters['b' + str(L)]\n",
        "    Z_out = linear_forward(A, W_out, b_out)  # Computing final output layer transformation\n",
        "    AL = Z_out  # Final output (activation not applied, assuming regression task)\n",
        "\n",
        "    return AL, caches  # Returning final output and stored computations\n"
      ],
      "metadata": {
        "id": "u1B2KQiSPs_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the first rowâ€™s cgpa and profile_score as input features.\n",
        "Reshaping the input into a column vector of shape (2,1)"
      ],
      "metadata": {
        "id": "JepQfVhSQG8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['cgpa', 'profile_score']].values[0].reshape(2, 1)\n"
      ],
      "metadata": {
        "id": "9JyVBk6uP8u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input layer: 2 neurons (for cgpa and profile_score)\n",
        "One hidden layer: 2 neurons\n",
        "Output layer: 1 neuron (for predicted salary)"
      ],
      "metadata": {
        "id": "EFbo4vjQQPVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = initialize_parameters([2, 2, 1])\n"
      ],
      "metadata": {
        "id": "c_CGbt7iQJVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing forward propagation to compute the predicted output (y_hat).\n",
        "y_hat, caches = L_layer_forward(X, parameters)\n",
        "print(\"Final output:\")\n",
        "print(y_hat)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUES2OvtQSEM",
        "outputId": "74e01783-15b8-4158-9cd7-847f5b7e0d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output:\n",
            "[[0.32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNTpyevVQhut"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1YmBZ540NhB9YcwnzPjvg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}